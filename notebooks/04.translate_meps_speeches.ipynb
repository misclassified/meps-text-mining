{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from utils import join_csv_files, split_text_into_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meps_list': {'output_dir': 'meps-text-mining/data/country_meps'},\n",
       " 'meps_speeches': {'europal_website': 'https://www.europarl.europa.eu',\n",
       "  'href_root': 'https://www.europarl.europa.eu/meps/en/',\n",
       "  'output_dir': 'meps-text-mining/data/meps_plenary_speeches',\n",
       "  'logs_dir': 'meps-text-mining/logs',\n",
       "  'logs_filename': 'scraping_speeches.txt'},\n",
       " 'meps_speeches_dataframe': {'output_dir': 'meps-text-mining/data/meps_speeches_dataframe',\n",
       "  'filename': 'meps_speeches.csv'},\n",
       " 'meps_speeches_en_translation': {'output_dir': 'meps-text-mining/data/meps_speeches_en_translations',\n",
       "  'filename': 'speeches_translations.txt',\n",
       "  'logs_filename': 'translating_speeches.txt'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../config/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all MEPS speeches available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "class HuggingfaceTranslator():\n",
    "\n",
    "    def __init__(self, model:str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model)  \n",
    "\n",
    "    @staticmethod\n",
    "    def split_text_into_chunks(text: str, max_tokens: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split a given text into chunks of manageable size based on the maximum number of tokens.\n",
    "        Splitting at punctuation and concatenating sentences if their total length is less than max tokens.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text to be split.\n",
    "        - max_tokens (int): The maximum number of tokens allowed per chunk.\n",
    "\n",
    "        Returns:\n",
    "        - List[str]: A list of text chunks.\n",
    "        \"\"\"\n",
    "        # Split the text into sentences using regex\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = len(sentence.split())\n",
    "            if len(current_chunk.split()) + sentence_tokens <= max_tokens:\n",
    "                current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "    \n",
    "\n",
    "    def translate(self, sample_text):\n",
    "    \n",
    "        batch = self.tokenizer([sample_text], return_tensors = \"pt\")\n",
    "        \n",
    "        generated_ids = self.model.generate(**batch)\n",
    "        tr = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        return tr\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(content_to_translate_df, outpath, translator):\n",
    "    \"\"\"Translation function\n",
    "    \n",
    "    Arguments:\n",
    "        content_to_translate_df: pandas dataframe\n",
    "        outpath: str\n",
    "            location where translations file is store\n",
    "        translator: class instance\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    for idx in content_to_translate_df.itertuples():\n",
    "        \n",
    "        url = idx.Url\n",
    "        content_to_translate = idx.Content\n",
    "\n",
    "        if isinstance(idx.Content, str):\n",
    "\n",
    "            try:\n",
    "                current_df = pd.read_csv(outpath, sep = '|', names= ['Url', 'Content'])\n",
    "            except:\n",
    "                print('No file created yet')\n",
    "                pass\n",
    "\n",
    "            if url in current_df['Url'].tolist():\n",
    "                logging.info(f\"\"\"{url} already_translated\"\"\")\n",
    "            else:\n",
    "                print(f\"\"\"translating {url}\"\"\")\n",
    "                logging.info(f\"\"\"translating {url}\"\"\")\n",
    "            \n",
    "                txt = translator.split_text_into_chunks(content_to_translate, max_tokens=150)\n",
    "\n",
    "                try:\n",
    "                    translated_text = \"\"\n",
    "\n",
    "                    for t in txt:\n",
    "                        trsl = translator.translate(t)\n",
    "                        translated_text += trsl[0]\n",
    "                except:\n",
    "                    logging.info(f\"\"\"Could not traslate {url}\"\"\")\n",
    "                    continue\n",
    "\n",
    "                with open(outpath, 'a') as file:\n",
    "                    # Write the tab-separated content to the file\n",
    "                    file.write('\\n' + '|'.join([url, translated_text]))\n",
    "        \n",
    "        else:\n",
    "            logging.info(f\"For {url} I couldn't find a content to translate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load speeches dataframe\n",
    "current_directory = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "file_name = os.path.join(config['meps_speeches_dataframe']['output_dir'], config['meps_speeches_dataframe']['filename'])\n",
    "speeches_df = pd.read_csv(os.path.join(os.path.dirname(current_directory), file_name))\n",
    "speeches_df.head()\n",
    "\n",
    "# Url must be unique as we are will be using it a key\n",
    "assert speeches_df['Url'].nunique() == len(speeches_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging \n",
    "\n",
    "logs_dir = os.path.join(os.path.dirname(current_directory), config['meps_speeches']['logs_dir'])\n",
    "logs_filename = config['meps_speeches_en_translation']['logs_filename']\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(logs_dir, logs_filename),\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speeches to translate\n",
    "speeches = speeches_df[speeches_df['Language'] == 'IT']\n",
    "\n",
    "out_path = os.path.join(os.path.dirname(current_directory), config['meps_speeches_en_translation']['output_dir'])\n",
    "file_path = os.path.join(out_path, config['meps_speeches_en_translation']['filename'])\n",
    "\n",
    "# Now reduce the speeches to the one that have not been translated yet\n",
    "translations = pd.read_csv(file_path, sep = '|', names = ['Url', 'Translation'])\n",
    "\n",
    "speeches_to_translate = speeches[~speeches['Url'].isin(translations['Url'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# Inizialize Translator Class\n",
    "model = \"Helsinki-NLP/opus-mt-it-en\"\n",
    "translator_class = HuggingfaceTranslator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2022-05-04-INT-3-065-0000_IT.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4342 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2023-12-12-INT-2-207-0000_IT.html\n",
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2024-01-17-INT-3-487-0000_IT.html\n",
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2024-02-07-INT-3-390-0000_IT.html\n",
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2024-01-15-INT-1-086-0000_IT.html\n",
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2024-01-18-INT-4-099-0000_IT.html\n",
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2024-01-16-INT-2-167-0000_IT.html\n",
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2023-12-14-INT-4-040-0000_IT.html\n",
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2024-02-06-INT-2-332-0000_IT.html\n",
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2024-01-16-INT-2-445-0000_IT.html\n",
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2024-02-08-INT-4-036-0000_IT.html\n",
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2024-01-17-INT-3-502-0000_IT.html\n",
      "translating https://www.europarl.europa.eu/doceo/document/CRE-9-2023-12-14-INT-4-017-0000_IT.html\n"
     ]
    }
   ],
   "source": [
    "# Translate Content passing the translator_class to the translator function\n",
    "translator(speeches_to_translate, file_path, translator_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
